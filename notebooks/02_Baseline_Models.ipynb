{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models: PCA & Isolation Forest\n",
    "\n",
    "**Objective**: Establish a performance baseline for anomaly detection using classical machine learning methods.\n",
    "**Comparison target**: LSTM Autoencoder (Notebook 06).\n",
    "\n",
    "## Models\n",
    "1.  **PCA (Principal Component Analysis)**: Detects anomalies based on high reconstruction error (Linear assumption).\n",
    "2.  **Isolation Forest**: Detects anomalies as data points that are \"few and different\" (Tree-based).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Auto-Detection: Local vs Kaggle ---\n",
    "# Strategy: If we can import 'src.config', we are in the Project Environment (Local/MinIO).\n",
    "# If we cannot, we are in a standalone Kernel (Kaggle).\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "try:\n",
    "    from src import config\n",
    "    IS_LOCAL_ENV = True\n",
    "    print(\"\u2705 Environment Detected: LOCAL (Project Structure Found)\")\n",
    "    \n",
    "    # Import Constants from Config\n",
    "    FEATURES = config.TENSOR_FEATURES\n",
    "    BUCKET_GOLD = getattr(config, \"BUCKET_GOLD\", \"gold\")\n",
    "    \n",
    "except ImportError:\n",
    "    IS_LOCAL_ENV = False\n",
    "    print(\"\ud83c\udf0d Environment Detected: KAGGLE (Standalone Kernel)\")\n",
    "    \n",
    "    # Hardcoded Fallbacks for Kaggle\n",
    "    FEATURES = [\n",
    "        \"dynamic_tensile_strength\", \"ejection\", \"tbl_speed\", \"cyl_main\", \"tbl_fill\"\n",
    "    ]\n",
    "    BUCKET_GOLD = \"gold\"\n",
    "\n",
    "# --- Data Loading Logic ---\n",
    "storage_options = None\n",
    "\n",
    "if IS_LOCAL_ENV:\n",
    "    # LOCAL: Use MinIO/S3 via pandas storage_options\n",
    "    # We assume 'src.config' implies S3 connectivity is desired/configured.\n",
    "    print(\"\u2601\ufe0f  Configuring S3/MinIO Connection...\")\n",
    "    DATA_DIR = f\"s3://{BUCKET_GOLD}\"\n",
    "    RESULTS_DIR = \"results/baseline\"\n",
    "    \n",
    "    storage_options = config.get_pandas_storage_options()\n",
    "        \n",
    "else:\n",
    "    # KAGGLE: Use Local Filesystem Paths\n",
    "    print(\"\ud83d\udcbe Configuring Local Filesystem Paths...\")\n",
    "    if os.path.exists(\"/kaggle/input/anomalydetection4pharma-gold-tensor/train.parquet\"):\n",
    "        DATA_DIR = \"/kaggle/input/anomalydetection4pharma-gold-tensor\"\n",
    "        RESULTS_DIR = \"/kaggle/working/results\"\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Using Data Dir: {DATA_DIR}\")\n",
    "print(f\"Using Results Dir: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "# Load Gold Parquet (Supports S3 via storage_options)\n",
    "try:\n",
    "    df_train = pd.read_parquet(f\"{DATA_DIR}/train.parquet\", storage_options=storage_options)[FEATURES]\n",
    "    df_test = pd.read_parquet(f\"{DATA_DIR}/test.parquet\", storage_options=storage_options)[FEATURES]\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"If using S3, check your credentials and endpoint.\")\n",
    "    print(\"If Local, check your path.\")\n",
    "    raise e\n",
    "\n",
    "print(f\"Train Shape: {df_train.shape}\")\n",
    "print(f\"Test Shape: {df_test.shape}\")\n",
    "\n",
    "# NOTE: Data is already MinMax Scaled [0, 1] from ETL.\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCA (Linear Reconstruction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "# Fit PCA\n",
    "N_COMPONENTS = 3\n",
    "\n",
    "pca = PCA(n_components=N_COMPONENTS)\n",
    "pca.fit(df_train)\n",
    "\n",
    "print(f\"Explained Variance per component: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total Explained Variance: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# Save Model\n",
    "joblib.dump(pca, os.path.join(RESULTS_DIR, 'pca_model.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "def get_pca_reconstruction_error(model, X):\n",
    "    X_pca = model.transform(X)\n",
    "    X_recon = model.inverse_transform(X_pca)\n",
    "    # MSE per sample\n",
    "    mse = np.mean(np.square(X - X_recon), axis=1)\n",
    "    return mse\n",
    "\n",
    "# Baseline Threshold (99.9 Percentile of Train)\n",
    "train_errors = get_pca_reconstruction_error(pca, df_train)\n",
    "THRESHOLD_PCA = np.percentile(train_errors, 99.9)\n",
    "print(f\"PCA Threshold (99.9%): {THRESHOLD_PCA:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Isolation Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "# Fit Isolation Forest\n",
    "# contamination='auto' or small value. \n",
    "# TUNING: Increasing contamination to 0.05 to force it to be more aggressive (and maybe catch anomalies).\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=42, n_jobs=-1)\n",
    "iso_forest.fit(df_train)\n",
    "\n",
    "# Save\n",
    "joblib.dump(iso_forest, os.path.join(RESULTS_DIR, 'iso_forest.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Stress Test (Synthetic Anomalies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "def run_baseline_stress_test(model, model_type, X_data, anomaly_type, magnitude=0.5, num_samples=200):\n",
    "    indices = np.random.choice(len(X_data), num_samples, replace=False)\n",
    "    results = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        row = X_data.iloc[idx].values.copy()\n",
    "        \n",
    "        # Inject Anomaly\n",
    "        if anomaly_type == 'spike':\n",
    "            row[0] += magnitude\n",
    "        elif anomaly_type == 'drift_end':\n",
    "            row[1] += magnitude\n",
    "        elif anomaly_type == 'freeze':\n",
    "            pass\n",
    "            \n",
    "        # Predict input wrapped in DataFrame\n",
    "        input_data = pd.DataFrame([row], columns=X_data.columns)\n",
    "        \n",
    "        detected = False\n",
    "        score = 0\n",
    "        \n",
    "        if model_type == 'pca':\n",
    "            error = get_pca_reconstruction_error(model, input_data)[0]\n",
    "            detected = error > THRESHOLD_PCA\n",
    "            score = error\n",
    "        elif model_type == 'if':\n",
    "            pred = model.predict(input_data)[0]\n",
    "            detected = (pred == -1)\n",
    "            score = model.decision_function(input_data)[0] * -1 # Invert so higher is more anomalous for plotting\n",
    "            \n",
    "        results.append({\n",
    "            'type': anomaly_type,\n",
    "            'model': model_type,\n",
    "            'magnitude': magnitude,\n",
    "            'detected': detected,\n",
    "            'score': score\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Running Sensitivity Analysis...\")\n",
    "MAGNITUDES = [0.1, 0.25, 0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "results_list = []\n",
    "\n",
    "for mag in MAGNITUDES:\n",
    "    print(f\"Testing Magnitude: {mag}\")\n",
    "    # PCA\n",
    "    results_list.append(run_baseline_stress_test(pca, 'pca', df_test, 'spike', magnitude=mag))\n",
    "    results_list.append(run_baseline_stress_test(pca, 'pca', df_test, 'drift_end', magnitude=mag))\n",
    "    \n",
    "    # IF\n",
    "    results_list.append(run_baseline_stress_test(iso_forest, 'if', df_test, 'spike', magnitude=mag))\n",
    "    results_list.append(run_baseline_stress_test(iso_forest, 'if', df_test, 'drift_end', magnitude=mag))\n",
    "\n",
    "# Freeze Tests (Single pass, magnitude 0.0)\n",
    "results_list.append(run_baseline_stress_test(pca, 'pca', df_test, 'freeze', magnitude=0.0))\n",
    "results_list.append(run_baseline_stress_test(iso_forest, 'if', df_test, 'freeze', magnitude=0.0))\n",
    "\n",
    "# Combine\n",
    "all_res = pd.concat(results_list)\n",
    "\n",
    "# Save Detailed Results\n",
    "all_res.to_csv(os.path.join(RESULTS_DIR, \"baseline_sensitivity.csv\"), index=False)\n",
    "print(f\"\\u2705 Saved detailed results to {RESULTS_DIR}/baseline_sensitivity.csv\")\n",
    "\n",
    "# Sensitivity Summary\n",
    "sens_summary = all_res.groupby(['model', 'type', 'magnitude'])['detected'].mean().reset_index()\n",
    "sens_summary['detection_rate'] = sens_summary['detected'] * 100\n",
    "print(sens_summary)\n",
    "sens_summary.to_csv(os.path.join(RESULTS_DIR, \"baseline_sensitivity_summary.csv\"), index=False)\n",
    "\n",
    "# --- Visualization: Sensitivity Curve ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=sens_summary[sens_summary['type'] != 'freeze'], x='magnitude', y='detection_rate', hue='model', style='type', markers=True, dashes=False)\n",
    "plt.axhline(100, color='green', linestyle='--', alpha=0.3)\n",
    "plt.title(\"Sensitivity Analysis: Detection Rate vs Anomaly Magnitude\")\n",
    "plt.ylabel(\"Detection Rate (%)\")\n",
    "plt.xlabel(\"Anomaly Magnitude (Sigma)\")\n",
    "plt.ylim(-5, 105)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"sensitivity_curve.png\"))\n",
    "print(\"\\u2705 Saved Sensitivity Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "**PCA** performs well on magnitude anomalies (Spike) but fails on temporal ones (Freeze) unless they break the linear correlation space significantly.\n",
    "**Isolation Forest** struggles with point-wise detection in this context without feature engineering (lagged windows).\n",
    "\n",
    "This justifies the need for **LSTM Autoencoder** (Notebook 06) which leverages temporal context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}