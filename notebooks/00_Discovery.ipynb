{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Phase 1: Forensic EDA (Dirty Data)\n",
                "\n",
                "**Objective**: Audit the raw \"Process time series\" data to identify structural issues before cleaning.\n",
                "\n",
                "**Key Checks**:\n",
                "1.  **Timestamp Audit**: Check for mixed formats (AM/PM vs 24h).\n",
                "2.  **Silence Detection**: Quantify \"Zero Speed\" rows (machine stops).\n",
                "3.  **Gap Analysis**: Identify interruptions in the 10s logging cadence.\n",
                "4.  **Integrity**: Check for fragmented batches.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Add src to path for config access\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
                "from src.config import get_boto3_client, get_pandas_storage_options\n",
                "\n",
                "# Configuration\n",
                "BUCKET = \"bronze\"\n",
                "PREFIX = \"process_time_series/\"\n",
                "\n",
                "s3 = get_boto3_client()\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Load Data Sample\n",
                "We will inspect a few files to gauge the quality.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "def get_csv_keys(bucket, prefix):\n",
                "    paginator = s3.get_paginator('list_objects_v2')\n",
                "    keys = []\n",
                "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
                "        if 'Contents' in page:\n",
                "            for obj in page['Contents']:\n",
                "                if obj['Key'].endswith('.csv'):\n",
                "                    keys.append(obj['Key'])\n",
                "    return keys\n",
                "\n",
                "csv_files = get_csv_keys(BUCKET, PREFIX)\n",
                "print(f\"Found {len(csv_files)} files.\")\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Load a sample file\n",
                "sample_key = csv_files[0]\n",
                "# Refactored to use centralized config\n",
                "s3_path = f\"s3://{BUCKET}/{sample_key}\"\n",
                "# Correctly using semicolon separator\n",
                "df = pd.read_csv(s3_path, sep=';', storage_options=get_pandas_storage_options())\n",
                "df.head()\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Check 1: Timestamp Formats\n",
                "Are there mixed AM/PM and 24h formats?\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "time_col = [c for c in df.columns if 'time' in c.lower()][0]\n",
                "print(f\"Time Column: {time_col}\")\n",
                "\n",
                "# Check for AM/PM\n",
                "has_ampm = df[time_col].astype(str).str.contains('AM|PM', case=False).any()\n",
                "print(f\"Has AM/PM: {has_ampm}\")\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Check 2: Silence (Zero Speed)\n",
                "How much data is \"dead air\" (machine stopped)?\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "speed_col = [c for c in df.columns if 'speed' in c.lower()][0]\n",
                "zeros = (df[speed_col] == 0).sum()\n",
                "pct = (zeros / len(df)) * 100\n",
                "print(f\"Zero Speed Rows: {zeros} ({pct:.2f}%)\")\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.plot(df[speed_col].values[:1000], label='Speed')\n",
                "plt.title(\"Process Speed (First 1000 rows)\")\n",
                "plt.legend()\n",
                "plt.show()\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Check 3: Gap Analysis\n",
                "Visualize the time differences between valid logs.\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "df['dt'] = pd.to_datetime(df[time_col], utc=True, errors='coerce')\n",
                "df = df.sort_values('dt')\n",
                "df['diff'] = df['dt'].diff().dt.total_seconds()\n",
                "\n",
                "# Filter for normal operation gaps (~10s)\n",
                "gaps = df['diff'].dropna()\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "sns.histplot(gaps[gaps < 60], bins=60)\n",
                "plt.title(\"Time Delta Histogram (Clipped at 60s)\")\n",
                "plt.xlabel(\"Seconds\")\n",
                "plt.show()\n",
                "\n",
                "print(\"Large Gaps (>1 min):\")\n",
                "print(gaps[gaps > 60].describe())\n"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. Conclusion & Action Plan\n",
                "\n",
                "**Findings (from Full Audit Script)**:\n",
                "1.  **Silence**: ~31% of data is Zero Speed. **Action**: Implement Rule 1 strict filtering.\n",
                "2.  **Gaps**: Major gaps confirmed (up to months). **Action**: We must split sequences on gaps > 10 min.\n",
                "3.  **Formats**: No mixed formats detected in this sample.\n",
                "\n",
                "**Next Step**: Use `etl_silver.py` to execute \"The Great Purge\".\n"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}