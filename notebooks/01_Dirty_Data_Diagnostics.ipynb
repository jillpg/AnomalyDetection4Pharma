{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 1: Dirty Data Diagnostics ðŸ©º\n",
    "\n",
    "**Goal**: Diagnose data quality issues (gaps, noise, format errors) in the Raw Bronze Layer (S3/MinIO) to define the \"Cleaning Rules\" for Phase 2.\n",
    "\n",
    "**Dataset Context**:\n",
    "*   **Source**: Nature Scientific Data (2022) - Pharma Manufacturing.\n",
    "*   **Components**:\n",
    "    1.  `Laboratory.csv`: Quality Targets (CQAs) & Metadata.\n",
    "    2.  `Process.csv`: Aggregated features (1 row per batch).\n",
    "    3.  `Process/*.csv`: **Raw Time Series** (Sensor data @ 10s frequency).\n",
    "\n",
    "## 1. Setup & Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T16:18:10.533335Z",
     "iopub.status.busy": "2026-01-21T16:18:10.533128Z",
     "iopub.status.idle": "2026-01-21T16:18:15.676881Z",
     "shell.execute_reply": "2026-01-21T16:18:15.675768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Configuring specific S3 endpoint for MinIO: http://minio:9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8ebfd4e2-aca4-497e-b108-9e7098ea2ed7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":: resolution report :: resolve 202ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8ebfd4e2-aca4-497e-b108-9e7098ea2ed7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n",
      "26/01/21 16:18:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 16:18:13 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Reading from Bucket: bronze\n"
     ]
    }
   ],
   "source": [
    "# Import Cloud-Agnostic Config\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from config import get_spark_session, get_data_path, get_boto3_client\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark\n",
    "spark = get_spark_session(\"DirtyEDA_Pharma\")\n",
    "\n",
    "bucket = os.getenv(\"BUCKET_BRONZE\", \"bronze\")\n",
    "print(f\"ðŸŒ Reading from Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingest_meta",
   "metadata": {},
   "source": [
    "## 2. Metadata & Aggregated Data (Batch Level)\n",
    "First, we check the metadata files (`Laboratory`, `Process` aggregated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_meta",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T16:18:15.679620Z",
     "iopub.status.busy": "2026-01-21T16:18:15.679333Z",
     "iopub.status.idle": "2026-01-21T16:18:22.393219Z",
     "shell.execute_reply": "2026-01-21T16:18:22.391751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 16:18:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Laboratory Count: 1005 rows (Batches)\n",
      "ðŸ“Š Process Aggregated Count: 1005 rows (Batches)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+--------------+\n",
      "|batch|code|dissolution_av|\n",
      "+-----+----+--------------+\n",
      "|    1|  25|         93.83|\n",
      "|    2|  25|         99.67|\n",
      "|    3|  25|         97.33|\n",
      "+-----+----+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Paths\n",
    "path_lab = f\"s3a://{bucket}/Laboratory.csv\"\n",
    "path_proc_agg = f\"s3a://{bucket}/Process.csv\"\n",
    "path_norm = f\"s3a://{bucket}/Normalization.csv\"\n",
    "\n",
    "# Load Data (Explicit Semicolon Delimiter for CSVs from these authors)\n",
    "df_lab = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").csv(path_lab)\n",
    "df_proc_agg = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").csv(path_proc_agg)\n",
    "\n",
    "print(f\"ðŸ“Š Laboratory Count: {df_lab.count()} rows (Batches)\")\n",
    "print(f\"ðŸ“Š Process Aggregated Count: {df_proc_agg.count()} rows (Batches)\")\n",
    "\n",
    "# Sample Check\n",
    "df_lab.select(\"batch\", \"code\", \"dissolution_av\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ingest_timeseries",
   "metadata": {},
   "source": [
    "## 3. Raw Time Series Analysis (`Process/*.csv`)\n",
    "This is the **High-Frequency Sensor Data** (100s of files). We load them all into a single DataFrame.\n",
    "\n",
    "*Note: In production, we assume keys are partitioned like `Process/file.csv`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load_ts",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T16:18:22.397952Z",
     "iopub.status.busy": "2026-01-21T16:18:22.397401Z",
     "iopub.status.idle": "2026-01-21T16:18:31.601640Z",
     "shell.execute_reply": "2026-01-21T16:18:31.600221Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:==================================>                       (3 + 2) / 5]\r",
      "\r",
      "[Stage 12:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:>                                                         (0 + 4) / 5]\r",
      "\r",
      "[Stage 13:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Time Series Total Rows: 4720208\n",
      "root\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- batch: integer (nullable = true)\n",
      " |-- code: integer (nullable = true)\n",
      " |-- tbl_speed: double (nullable = true)\n",
      " |-- fom: double (nullable = true)\n",
      " |-- main_comp: double (nullable = true)\n",
      " |-- tbl_fill: double (nullable = true)\n",
      " |-- SREL: double (nullable = true)\n",
      " |-- pre_comp: double (nullable = true)\n",
      " |-- produced: integer (nullable = true)\n",
      " |-- waste: integer (nullable = true)\n",
      " |-- cyl_main: double (nullable = true)\n",
      " |-- cyl_pre: double (nullable = true)\n",
      " |-- stiffness: integer (nullable = true)\n",
      " |-- ejection: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:==============================================>           (4 + 1) / 5]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path_ts = f\"s3a://{bucket}/Process/*.csv\"\n",
    "\n",
    "# Load ALL Time Series files\n",
    "# Note: The user confirmed TS files also use semicolon delimiter.\n",
    "\n",
    "try:\n",
    "    df_ts = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").csv(path_ts)\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading TS: {e}\")\n",
    "\n",
    "print(f\"ðŸ“ˆ Time Series Total Rows: {df_ts.count()}\")\n",
    "df_ts.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gap_analysis",
   "metadata": {},
   "source": [
    "## 4. Gap Analysis (Temporal Continuity)\n",
    "We need to find missing data points in the Time Series.\n",
    "*   **Expected Frequency**: 10 seconds.\n",
    "*   **Risk**: If Delta > 10s, we have a gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "calc_gaps",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T16:18:31.604209Z",
     "iopub.status.busy": "2026-01-21T16:18:31.603988Z",
     "iopub.status.idle": "2026-01-21T16:18:42.924322Z",
     "shell.execute_reply": "2026-01-21T16:18:42.923635Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Found Timestamp Column: timestamp\n",
      "âš ï¸ Top 5 Gaps (Seconds):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 16:18:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 16:18:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 16:18:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 16:18:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "26/01/21 16:18:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----+----+---------+---+---------+--------+----+--------+--------+-----+--------+-------+---------+--------+-------------------+-------------------+---------+\n",
      "|          timestamp|campaign|batch|code|tbl_speed|fom|main_comp|tbl_fill|SREL|pre_comp|produced|waste|cyl_main|cyl_pre|stiffness|ejection|          ts_parsed|            prev_ts|delta_sec|\n",
      "+-------------------+--------+-----+----+---------+---+---------+--------+----+--------+--------+-----+--------+-------+---------+--------+-------------------+-------------------+---------+\n",
      "|2018-11-13 06:16:33|       3|    1|  25|      0.0|0.0|      0.8|    5.68| 0.0|     0.0|       0|    0|    1.79|    6.0|       50|     107|2018-11-13 06:16:33|2018-06-12 22:10:55| 13248338|\n",
      "|2020-04-28 21:40:41|     131|  622|  21|      0.0|0.0|      4.6|    6.01| 6.0|     0.0|       0|    0|     1.9|    8.0|       41|     177|2020-04-28 21:40:41|2020-01-30 17:48:56|  7703505|\n",
      "|2020-08-06 02:42:01|     135|  667|  21|      0.0|0.0|      4.5|     5.5| 0.0|     0.5|       0|    0|     1.8|    6.0|       46|     182|2020-08-06 02:42:01|2020-06-08 02:19:19|  5098962|\n",
      "|2020-01-30 06:55:54|     119|  557|  21|      0.0|0.0|      0.0|    5.68| 0.0|     0.0|       0|    0|    1.79|    6.0|        0|       0|2020-01-30 06:55:54|2019-12-20 11:12:46|  3526988|\n",
      "|2018-06-12 04:12:33|      13|   67|  10|      0.0|0.0|      8.9|    5.39| 0.0|     0.0|       0|    0|    1.05|    2.5|        9|     156|2018-06-12 04:12:33|2018-05-12 18:50:16|  2625737|\n",
      "+-------------------+--------+-----+----+---------+---+---------+--------+----+--------+--------+-----+--------+-------+---------+--------+-------------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ensure 'time_stamp' column exists (name varies in raw data: 'Time', 'timestamp', 'time_stamp')\n",
    "# We'll search for it.\n",
    "ts_col = [c for c in df_ts.columns if 'time' in c.lower()]\n",
    "if ts_col:\n",
    "    target_ts_col = ts_col[0]\n",
    "    print(f\"â±ï¸ Found Timestamp Column: {target_ts_col}\")\n",
    "    \n",
    "    # Cast to timestamp\n",
    "    df_ts_clean = df_ts.withColumn(\"ts_parsed\", F.to_timestamp(F.col(target_ts_col)))\n",
    "    \n",
    "    # Window for Lag\n",
    "    # Note: Raw TS might not have 'batch' column inside. If not, we rely on file input_file_name() or just assume continuity per file.\n",
    "    # For Dirty EDA, we analyze global gaps.\n",
    "    w = Window.orderBy(\"ts_parsed\")\n",
    "    \n",
    "    df_gaps = df_ts_clean.withColumn(\"prev_ts\", F.lag(\"ts_parsed\").over(w)) \\\n",
    "        .withColumn(\"delta_sec\", F.col(\"ts_parsed\").cast(\"long\") - F.col(\"prev_ts\").cast(\"long\"))\n",
    "    \n",
    "    print(\"âš ï¸ Top 5 Gaps (Seconds):\")\n",
    "    df_gaps.filter(\"delta_sec > 11\").orderBy(F.desc(\"delta_sec\")).show(5)\n",
    "else:\n",
    "    print(\"âŒ No Timestamp column found in Time Series schema!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noise_profile",
   "metadata": {},
   "source": [
    "## 5. Noise & Outlier Profiling\n",
    "Focus on Critical Process Parameters (CPPs).\n",
    "*   **Target**: `main_comp` (Compression Force in kN).\n",
    "*   **Rules**: No Negatives. Check 5-Sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "check_noise",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T16:18:42.927437Z",
     "iopub.status.busy": "2026-01-21T16:18:42.927196Z",
     "iopub.status.idle": "2026-01-21T16:18:53.857897Z",
     "shell.execute_reply": "2026-01-21T16:18:53.856761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Analyzing Sensor: main_comp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                         (0 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:=======================>                                  (2 + 3) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âŒ Negative Values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:=======================>                                  (2 + 3) / 5]\r",
      "\r",
      "[Stage 22:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:=======================>                                  (2 + 3) / 5]\r",
      "\r",
      "[Stage 28:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ðŸ“‰ Outliers (> 5Ïƒ): 2499 / 4720208\n",
      "   ðŸ“Š Mean: 6.21, StdDev: 2.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find Main Compression Column\n",
    "# Likely 'main_comp' or similar\n",
    "comp_cols = [c for c in df_ts.columns if 'main' in c.lower() and 'comp' in c.lower()]\n",
    "\n",
    "if comp_cols:\n",
    "    target_col = comp_cols[0]\n",
    "    print(f\"ðŸŽ¯ Analyzing Sensor: {target_col}\")\n",
    "    \n",
    "    # 1. Cast to Double (Crucial explicitly)\n",
    "    df_ts = df_ts.withColumn(target_col, F.col(target_col).cast(\"double\"))\n",
    "    \n",
    "    # 2. Negative Values (Physical Impossibility)\n",
    "    neg_count = df_ts.filter(F.col(target_col) < 0).count()\n",
    "    print(f\"   âŒ Negative Values: {neg_count}\")\n",
    "    \n",
    "    # 3. Spikes (5 Sigma)\n",
    "    stats = df_ts.select(F.mean(target_col).alias(\"mu\"), F.stddev(target_col).alias(\"sigma\")).collect()[0]\n",
    "    \n",
    "    if stats['mu'] is not None and stats['sigma'] is not None:\n",
    "        upper = stats['mu'] + 5 * stats['sigma']\n",
    "        lower = stats['mu'] - 5 * stats['sigma']\n",
    "        \n",
    "        outliers = df_ts.filter((F.col(target_col) > upper) | (F.col(target_col) < lower)).count()\n",
    "        print(f\"   ðŸ“‰ Outliers (> 5Ïƒ): {outliers} / {df_ts.count()}\")\n",
    "        print(f\"   ðŸ“Š Mean: {stats['mu']:.2f}, StdDev: {stats['sigma']:.2f}\")\n",
    "    else:\n",
    "        print(\"   âš ï¸ Stats is None. Checking data quality...\")\n",
    "        df_ts.select(target_col).show(5)\n",
    "else:\n",
    "    print(\"âŒ Main Compression column not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manifesto",
   "metadata": {},
   "source": [
    "## 6. The Cleaning Manifesto ðŸ“œ\n",
    "\n",
    "**Data-Driven Rules for Phase 2 (ETL Pipeline)**:\n",
    "\n",
    "| ID | Data Quality Issue | Findings in Bronze | Action Rule | Severity |\n",
    "|----|--------------------|--------------------|-------------|----------|\n",
    "| **R1** | **Negative Pressures** | **0 found**. Data is physically consistent. | **Verify & Monitor**. (Do not implement aggressive drops yet). | Low |\n",
    "| **R2** | **Temporal Gaps** | Huge gaps found (> 10M sec) between campaigns. | **Split Sequences**. Use `campaign` ID as the primary partition key. | **Critical** |\n",
    "| **R3** | **Sensor Outliers** | ~2,500 points > 5$\\sigma$ (0.05% of data). | **Cap/Clip** at $\\mu \\pm 5\\sigma$ or use Robust Scaler. | Medium |\n",
    "| **R4** | **Schema** | Delimiters vary (`;` vs `,`). | **Enforce `;`** for all Source-of-Truth reading. | High |\n",
    "| **R5** | **Normalization** | Variable scales differ (kN vs RPM). | **StandardScaler** (Z-Score) required for Deep Learning. | High |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
