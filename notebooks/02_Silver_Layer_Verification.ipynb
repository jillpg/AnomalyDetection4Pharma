{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Phase 2: Silver Layer Verification ü•à\n",
    "\n",
    "**Goal**: Validate that the Bronze -> Silver ETL Job worked correctly.\n",
    "\n",
    "**Checklist**:\n",
    "1.  **Format**: Is it Parquet?\n",
    "2.  **Schema**: Are sensors now `DoubleType`? (No more strings)\n",
    "3.  **Partitions**: Is it partitioned by `campaign`?\n",
    "4.  **Cleaning**: Are outliers clipped? Are gaps handled (`sequence_id`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T17:47:11.696100Z",
     "iopub.status.busy": "2026-01-21T17:47:11.695862Z",
     "iopub.status.idle": "2026-01-21T17:47:17.497556Z",
     "shell.execute_reply": "2026-01-21T17:47:17.495835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuring specific S3 endpoint for MinIO: http://minio:9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.11/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-758abdd4-65d3-4421-a0e1-2cf1cf6d33fe;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 242ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-758abdd4-65d3-4421-a0e1-2cf1cf6d33fe\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 17:47:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 17:47:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Inspecting: s3a://silver\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from config import get_spark_session, get_data_path\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ensure Spark knows where Python is (Safety Check)\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = get_spark_session(\"SilverVerification\")\n",
    "silver_path = get_data_path(\"silver\")\n",
    "\n",
    "print(f\"üîç Inspecting: {silver_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "read_parquet",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T17:47:17.500585Z",
     "iopub.status.busy": "2026-01-21T17:47:17.500274Z",
     "iopub.status.idle": "2026-01-21T17:47:26.990040Z",
     "shell.execute_reply": "2026-01-21T17:47:26.988823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 17:47:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                        (0 + 4) / 187]\r",
      "\r",
      "[Stage 0:==>                                                      (8 + 4) / 187]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:=====>                                                  (19 + 4) / 187]\r",
      "\r",
      "[Stage 0:=========>                                              (31 + 4) / 187]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:=============>                                          (45 + 4) / 187]\r",
      "\r",
      "[Stage 0:==================>                                     (62 + 4) / 187]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:=======================>                                (80 + 5) / 187]\r",
      "\r",
      "[Stage 0:===========================>                            (93 + 4) / 187]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:================================>                      (112 + 5) / 187]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:========================================>              (139 + 4) / 187]\r",
      "\r",
      "[Stage 0:=================================================>     (168 + 4) / 187]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 4) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:========>                                                  (1 + 4) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:==========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total Rows: 4,720,208\n",
      "üìã Schema Validation:\n",
      "root\n",
      " |-- batch: integer (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- code: integer (nullable = true)\n",
      " |-- tbl_speed: double (nullable = true)\n",
      " |-- fom: double (nullable = true)\n",
      " |-- main_comp: double (nullable = true)\n",
      " |-- tbl_fill: double (nullable = true)\n",
      " |-- SREL: double (nullable = true)\n",
      " |-- pre_comp: double (nullable = true)\n",
      " |-- produced: integer (nullable = true)\n",
      " |-- waste: integer (nullable = true)\n",
      " |-- cyl_main: double (nullable = true)\n",
      " |-- cyl_pre: double (nullable = true)\n",
      " |-- stiffness: integer (nullable = true)\n",
      " |-- ejection: double (nullable = true)\n",
      " |-- sequence_id: long (nullable = true)\n",
      " |-- strength: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      " |-- start: string (nullable = true)\n",
      " |-- api_code: integer (nullable = true)\n",
      " |-- api_batch: integer (nullable = true)\n",
      " |-- smcc_batch: integer (nullable = true)\n",
      " |-- lactose_batch: integer (nullable = true)\n",
      " |-- starch_batch: integer (nullable = true)\n",
      " |-- api_water: string (nullable = true)\n",
      " |-- api_total_impurities: string (nullable = true)\n",
      " |-- api_l_impurity: string (nullable = true)\n",
      " |-- api_content: double (nullable = true)\n",
      " |-- api_ps01: string (nullable = true)\n",
      " |-- api_ps05: string (nullable = true)\n",
      " |-- api_ps09: string (nullable = true)\n",
      " |-- lactose_water: double (nullable = true)\n",
      " |-- lactose_sieve0045: integer (nullable = true)\n",
      " |-- lactose_sieve015: integer (nullable = true)\n",
      " |-- lactose_sieve025: integer (nullable = true)\n",
      " |-- smcc_water: double (nullable = true)\n",
      " |-- smcc_td: double (nullable = true)\n",
      " |-- smcc_bd: double (nullable = true)\n",
      " |-- smcc_ps01: double (nullable = true)\n",
      " |-- smcc_ps05: double (nullable = true)\n",
      " |-- smcc_ps09: double (nullable = true)\n",
      " |-- starch_ph: double (nullable = true)\n",
      " |-- starch_water: double (nullable = true)\n",
      " |-- tbl_min_thickness: double (nullable = true)\n",
      " |-- tbl_max_thickness: double (nullable = true)\n",
      " |-- fct_min_thickness: double (nullable = true)\n",
      " |-- fct_max_thickness: double (nullable = true)\n",
      " |-- tbl_min_weight: double (nullable = true)\n",
      " |-- tbl_max_weight: double (nullable = true)\n",
      " |-- tbl_rsd_weight: double (nullable = true)\n",
      " |-- fct_rsd_weight: double (nullable = true)\n",
      " |-- tbl_min_hardness: double (nullable = true)\n",
      " |-- tbl_max_hardness: double (nullable = true)\n",
      " |-- tbl_av_hardness: integer (nullable = true)\n",
      " |-- fct_min_hardness: double (nullable = true)\n",
      " |-- fct_max_hardness: double (nullable = true)\n",
      " |-- fct_av_hardness: double (nullable = true)\n",
      " |-- tbl_max_diameter: double (nullable = true)\n",
      " |-- fct_max_diameter: double (nullable = true)\n",
      " |-- tbl_tensile: double (nullable = true)\n",
      " |-- fct_tensile: double (nullable = true)\n",
      " |-- tbl_yield: double (nullable = true)\n",
      " |-- batch_yield: double (nullable = true)\n",
      " |-- dissolution_av: double (nullable = true)\n",
      " |-- dissolution_min: integer (nullable = true)\n",
      " |-- resodual_solvent: double (nullable = true)\n",
      " |-- impurities_total: double (nullable = true)\n",
      " |-- impurity_o: double (nullable = true)\n",
      " |-- impurity_l: double (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load Silver Data (Parquet)\n",
    "path = f\"{silver_path}/Process\"\n",
    "df_silver = spark.read.parquet(path)\n",
    "\n",
    "print(f\"üìä Total Rows: {df_silver.count():,}\")\n",
    "print(\"üìã Schema Validation:\")\n",
    "df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_types",
   "metadata": {},
   "source": [
    "## 1. Type Enforcement Check\n",
    "We need to ensure `main_comp`, `pre_comp` etc. are `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "check_types",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T17:47:26.993618Z",
     "iopub.status.busy": "2026-01-21T17:47:26.993191Z",
     "iopub.status.idle": "2026-01-21T17:47:27.007724Z",
     "shell.execute_reply": "2026-01-21T17:47:27.006415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Type Check Passed: All sensors are DoubleType.\n"
     ]
    }
   ],
   "source": [
    "dtypes = dict(df_silver.dtypes)\n",
    "assert dtypes['main_comp'] == 'double', \"‚ùå Main Comp should be Double!\"\n",
    "assert dtypes['tbl_speed'] == 'double', \"‚ùå Tbl Speed should be Double!\"\n",
    "print(\"‚úÖ Type Check Passed: All sensors are DoubleType.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_cleaning",
   "metadata": {},
   "source": [
    "## 2. Cleaning Verification (Outliers & Gaps)\n",
    "Check if `sequence_id` exists and outlier clipping worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "check_cleaning",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T17:47:27.011214Z",
     "iopub.status.busy": "2026-01-21T17:47:27.010887Z",
     "iopub.status.idle": "2026-01-21T17:47:30.637776Z",
     "shell.execute_reply": "2026-01-21T17:47:30.636625Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 4) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:================>                                          (2 + 4) / 7]\r",
      "\r",
      "[Stage 5:==========================================>                (5 + 2) / 7]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sequence Spitting logic applied. Found 91 distinct continuous sequences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 4) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Main Compression Range: [0.00, 16.86]\n",
      "‚úÖ Outlier Clipping appears effective (Values within reasonable 5-sigma range).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 1. Gaps (Sequence ID)\n",
    "if 'sequence_id' in df_silver.columns:\n",
    "    files_count = df_silver.select(\"sequence_id\").distinct().count()\n",
    "    print(f\"‚úÖ Sequence Spitting logic applied. Found {files_count} distinct continuous sequences.\")\n",
    "else:\n",
    "    print(\"‚ùå sequence_id column MISSING!\")\n",
    "\n",
    "# 2. Outliers (Max/Min Check)\n",
    "stats = df_silver.select(F.min(\"main_comp\"), F.max(\"main_comp\")).collect()[0]\n",
    "print(f\"üìâ Main Compression Range: [{stats[0]:.2f}, {stats[1]:.2f}]\")\n",
    "\n",
    "# Note: In the ETL log we saw clipping at (-4.44, 16.86). Values should be within this.\n",
    "if stats[1] <= 17.0 and stats[0] >= -5.0:\n",
    "    print(\"‚úÖ Outlier Clipping appears effective (Values within reasonable 5-sigma range).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Values outside expected clip range. Check ETL logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify_enrichment",
   "metadata": {},
   "source": [
    "## 3. Enrichment (Lab Data)\n",
    "Did we successfully join the `Laboratory.csv` labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "check_enrichment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T17:47:30.640248Z",
     "iopub.status.busy": "2026-01-21T17:47:30.640038Z",
     "iopub.status.idle": "2026-01-21T17:47:31.041912Z",
     "shell.execute_reply": "2026-01-21T17:47:31.041242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/21 17:47:30 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Enrichment Successful. Example Dissolution: 93.0 (Batch: 241)\n"
     ]
    }
   ],
   "source": [
    "# Check for a Lab specific column, e.g., 'dissolution_av'\n",
    "if 'dissolution_av' in df_silver.columns:\n",
    "    row = df_silver.filter(\"dissolution_av IS NOT NULL\").first()\n",
    "    if row:\n",
    "        print(f\"‚úÖ Data Enrichment Successful. Example Dissolution: {row['dissolution_av']} (Batch: {row.batch})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Column exists but all values are Null. Check Join Link!\")\n",
    "else:\n",
    "    print(\"‚ùå Lab columns missing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
