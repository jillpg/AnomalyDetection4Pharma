{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Model Evaluation & Stress Testing (Pro Edition)\n",
    "\n",
    "**Objective**: Rigorous validation of the LSTM Autoencoder using Statistical Stress Testing.\n",
    "\n",
    "**Methodology**:\n",
    "Instead of testing a single anomaly, we use a **Monte Carlo approach**:\n",
    "1.  **Baseline**: Establish the Anomaly Threshold from the Test Set (Mean + 3 StdDev).\n",
    "2.  **Injection**: For each scenario, we inject failures into **100 random windows**.\n",
    "3.  **Metrics**: We calculate:\n",
    "    *   **Detection Rate**: % of anomalies caught.\n",
    "    *   **SNR (Signal-to-Noise Ratio)**: Ratio of Anomaly Error to Normal Error.\n",
    "\n",
    "**Scenarios**:\n",
    "1.  **Spike**: Sudden jump in Tensile Strength (+50%).\n",
    "2.  **Drift**: Gradual increase in Ejection Force (Sticking).\n",
    "3.  **Freeze**: Sensor flatline (Data loss).\n",
    "\n",
    "**Outputs**:\n",
    "*   `stress_test_summary.json`: Detailed metrics.\n",
    "*   `stress_test_results.csv`: Raw data of the tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# --- Auto-Detection: Local vs Kaggle ---\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "try:\n",
    "    from src import config\n",
    "    IS_LOCAL_ENV = True\n",
    "    print(\"\u2705 Environment Detected: LOCAL (Project Structure Found)\")\n",
    "    RESULTS_DIR = \"results/stress\"\n",
    "    DATA_DIR = getattr(config, \"BUCKET_GOLD\", \"gold\")\n",
    "except ImportError:\n",
    "    IS_LOCAL_ENV = False\n",
    "    print(\"\ud83c\udf0d Environment Detected: KAGGLE (Standalone Kernel)\")\n",
    "    RESULTS_DIR = \"/kaggle/working\"\n",
    "    DATA_DIR = \"/kaggle/input/anomalydetection4pharma-gold-tensor\"\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(f\"Using Results Dir: {RESULTS_DIR}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Class Definitions (Embedded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "def create_windows(df, window_size=60, stride=5, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        raise ValueError(\"Must provide feature_cols list\")\n",
    "    X = []\n",
    "    if 'batch' in df.columns:\n",
    "        grouped = df.groupby('batch')\n",
    "        for batch_id, group in grouped:\n",
    "            group = group.sort_values('timestamp')\n",
    "            data = group[feature_cols].values\n",
    "            num_samples = len(data)\n",
    "            if num_samples < window_size:\n",
    "                continue\n",
    "            for i in range(0, num_samples - window_size + 1, stride):\n",
    "                window = data[i : i + window_size]\n",
    "                X.append(window)\n",
    "    else:\n",
    "        data = df[feature_cols].values\n",
    "        num_samples = len(data)\n",
    "        for i in range(0, num_samples - window_size + 1, stride):\n",
    "            window = data[i : i + window_size]\n",
    "            X.append(window)\n",
    "    return np.array(X)\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Autoencoder for Anomaly Detection in Time Series (PyTorch Version).\n",
    "    Architecture:\n",
    "        Input(Window) -> Encoder(LSTM) -> Latent(Vector) -> Decoder(LSTM) -> Output(Window)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size, n_features, latency_dim=3, hidden_dim=64, num_layers=1, dropout=0.2):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.n_features = n_features\n",
    "        self.latency_dim = latency_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=n_features, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "        # Latent compression\n",
    "        self.to_latent = nn.Linear(hidden_dim, latency_dim)\n",
    "        self.from_latent = nn.Linear(latency_dim, hidden_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, n_features)\n",
    "        \n",
    "        # Initialize Weights\n",
    "        self._init_weights()\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Xavier Initialization for better convergence.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        _, (hidden_n, _) = self.encoder(x)\n",
    "        last_hidden = hidden_n[-1]\n",
    "        last_hidden = self.dropout_layer(last_hidden) # Apply Dropout\n",
    "        \n",
    "        # Latent\n",
    "        latent = self.to_latent(last_hidden)\n",
    "        latent = nn.functional.leaky_relu(latent) # Non-linearity for complex patterns\n",
    "        \n",
    "        # Decoder Prep\n",
    "        hidden_restored = self.from_latent(latent)\n",
    "        # Repeat for each time step\n",
    "        repeated_hidden = hidden_restored.unsqueeze(1).repeat(1, self.window_size, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_out, _ = self.decoder(repeated_hidden)\n",
    "        \n",
    "        # Reconstruction\n",
    "        reconstructed = self.output_layer(dec_out)\n",
    "        return reconstructed, latent # Return latent for visualization\n",
    "\n",
    "    def save_checkpoint(self, path, epoch, optimizer, loss):\n",
    "        \"\"\"Saves full checkpoint with metadata.\"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            'config': {\n",
    "                'window_size': self.window_size,\n",
    "                'n_features': self.n_features, \n",
    "                'hidden_dim': self.hidden_dim,\n",
    "                'latent_dim': self.latency_dim\n",
    "            }\n",
    "        }, path)\n",
    "        print(f\"  \ud83d\udcbe Checkpoint saved: {path}\")\n",
    "\n",
    "    def train_model(self, X_train, X_val, epochs=50, batch_size=64, lr=1e-3, \n",
    "                    patience=15, scheduler_patience=3, scheduler_factor=0.5, \n",
    "                    save_path=\"models/lstm_ae_champion.pth\", noise_factor=0.0):\n",
    "        \"\"\"\n",
    "        Trains the model with Limit-aware Learning Rate Scheduler and Early Stopping.\n",
    "        Uses Lazy Loading and Validation Batching to prevent OOM.\n",
    "        \"\"\"\n",
    "        # Convert to Tensor but KEEP ON CPU initially\n",
    "        train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
    "        val_tensor = torch.from_numpy(X_val.astype(np.float32))\n",
    "        \n",
    "        train_loader = DataLoader(TensorDataset(train_tensor, train_tensor), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TensorDataset(val_tensor, val_tensor), batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=scheduler_factor, patience=scheduler_patience, verbose=True\n",
    "        )\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        counter = 0\n",
    "        history = {'train_loss': [], 'val_loss': []}\n",
    "        \n",
    "        print(f\"\ud83d\ude80 Device: {self.device}\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch_x, target_x in train_loader:\n",
    "                # Move to GPU Just-In-Time\n",
    "                batch_x = batch_x.to(self.device)\n",
    "                target_x = target_x.to(self.device)\n",
    "\n",
    "                # Apply Denoising Noise (if enabled)\n",
    "                if noise_factor > 0:\n",
    "                    noise = torch.randn_like(batch_x) * noise_factor\n",
    "                    batch_x = batch_x + noise\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output, _ = self(batch_x)\n",
    "                loss = criterion(output, target_x)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Validation (Batched)\n",
    "            self.eval()\n",
    "            val_loss_accum = 0\n",
    "            with torch.no_grad():\n",
    "                for val_batch, _ in val_loader:\n",
    "                   val_batch = val_batch.to(self.device)\n",
    "                   val_out, _ = self(val_batch)\n",
    "                   batch_loss = criterion(val_out, val_batch).item()\n",
    "                   val_loss_accum += batch_loss\n",
    "            \n",
    "            avg_val_loss = val_loss_accum / len(val_loader)\n",
    "            \n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "            \n",
    "            # Update Scheduler\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # Checkpoint\n",
    "            if avg_val_loss < best_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                self.save_checkpoint(save_path, epoch, optimizer, best_loss)\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    print(f\"\ud83d\uded1 Early Stopping triggered after {patience} epochs without improvement.\")\n",
    "                    break\n",
    "                    \n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.eval()\n",
    "        tensor_X = torch.from_numpy(X.astype(np.float32)).to(self.device)\n",
    "        dataset = TensorDataset(tensor_X)\n",
    "        loader = DataLoader(dataset, batch_size=256, shuffle=False)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch_x = batch[0]\n",
    "                out, _ = self(batch_x)\n",
    "                predictions.append(out.cpu().numpy())\n",
    "                \n",
    "        return np.concatenate(predictions, axis=0)\n",
    "    \n",
    "    def get_reconstruction_error(self, X):\n",
    "        \"\"\"\n",
    "        Calculates MSE for each sample window.\n",
    "        Returns: Array of shape (n_samples,) containing the MSE score.\n",
    "        \"\"\"\n",
    "        X_pred = self.predict(X)\n",
    "        mse = np.mean(np.square(X - X_pred), axis=(1, 2))\n",
    "        return mse\n",
    "\n",
    "\n",
    "# --- PHASE 8 ENGINEERING LOGIC ---\n",
    "\n",
    "\n",
    "def get_reconstruction_errors(model, X):\n",
    "    model.eval()\n",
    "    tensor_X = torch.from_numpy(X.astype(np.float32)).to(DEVICE)\n",
    "    errors = []\n",
    "    batch_size = 256\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch = tensor_X[i:i+batch_size]\n",
    "            reconstructed, _ = model(batch)\n",
    "            # MAE per sample (avg over window_size and n_features)\n",
    "            mae_loss = torch.mean(torch.abs(batch - reconstructed), dim=(1, 2))\n",
    "            errors.append(mae_loss.cpu().numpy())\n",
    "    return np.concatenate(errors)\n",
    "\n",
    "def check_freeze(window, epsilon=1e-4):\n",
    "    \"\"\"Rule-based Freeze Detection: Checks if signal is flat.\"\"\"\n",
    "    std_devs = np.std(window, axis=0)\n",
    "    # If ANY feature has almost 0 std dev, it's a freeze\n",
    "    return np.any(std_devs < epsilon)\n",
    "\n",
    "def get_latent_vectors(model, X_loader):\n",
    "    \"\"\"Extracts latent vectors (bottleneck).\"\"\"\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch in X_loader:\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            _, latent = model(batch)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "    return np.concatenate(latents, axis=0)\n",
    "\n",
    "def fit_mahalanobis(latents):\n",
    "    \"\"\"Calculates Mean and Inverse Covariance Matrix.\"\"\"\n",
    "    mu = np.mean(latents, axis=0)\n",
    "    cov = np.cov(latents.T)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    return mu, inv_cov\n",
    "\n",
    "def calculate_mahalanobis(x, mu, inv_cov):\n",
    "    \"\"\"Calculates Mahalanobis distance for a single vector x.\"\"\"\n",
    "    delta = x - mu\n",
    "    m_dist = np.sqrt(np.dot(np.dot(delta, inv_cov), delta.T))\n",
    "    return m_dist\n",
    "\n",
    "\n",
    "# --- PHASE 8 ENGINEERING LOGIC ---\n",
    "\n",
    "def check_freeze(window, epsilon=1e-4):\n",
    "    \"\"\"Rule-based Freeze Detection: Checks if signal is flat.\"\"\"\n",
    "    std_devs = np.std(window, axis=0)\n",
    "    # If ANY feature has almost 0 std dev, it's a freeze\n",
    "    return np.any(std_devs < epsilon)\n",
    "\n",
    "def get_latent_vectors(model, X_loader):\n",
    "    \"\"\"Extracts latent vectors (bottleneck).\"\"\"\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch in X_loader:\n",
    "            batch = batch[0].to(DEVICE)\n",
    "            _, latent = model(batch)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "    return np.concatenate(latents, axis=0)\n",
    "\n",
    "def fit_mahalanobis(latents):\n",
    "    \"\"\"Calculates Mean and Inverse Covariance Matrix.\"\"\"\n",
    "    mu = np.mean(latents, axis=0)\n",
    "    cov = np.cov(latents.T)\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    return mu, inv_cov\n",
    "\n",
    "def calculate_mahalanobis(x, mu, inv_cov):\n",
    "    \"\"\"Calculates Mahalanobis distance for a single vector x.\"\"\"\n",
    "    delta = x - mu\n",
    "    m_dist = np.sqrt(np.dot(np.dot(delta, inv_cov), delta.T))\n",
    "    return m_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model & Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "# Load Config\n",
    "try:\n",
    "    with open(os.path.join(RESULTS_DIR, \"model_config.pkl\"), \"rb\") as f:\n",
    "        config = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    config = {'window_size': 60, 'n_features': 5, 'hidden_dim': 64, 'latent_dim': 3}\n",
    "\n",
    "# Load Weights\n",
    "weight_files = glob.glob(os.path.join(RESULTS_DIR, \"*.pth\"))\n",
    "model_path = weight_files[0]\n",
    "model = LSTMAutoencoder(window_size=config['window_size'], n_features=config['n_features'],\n",
    "                        hidden_dim=config['hidden_dim'], latency_dim=config['latent_dim'])\n",
    "checkpoint = torch.load(model_path, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Load Data\n",
    "df_test = pd.read_parquet(os.path.join(DATA_DIR, \"test.parquet\")).sort_values(\"timestamp\")\n",
    "df_train = pd.read_parquet(os.path.join(DATA_DIR, \"train.parquet\")).sort_values(\"timestamp\") # ADDED FOR THRESHOLDING\n",
    "\n",
    "X_test = create_windows(df_test, config['window_size'], stride=5, feature_cols=config['features'])\n",
    "# Subsample Train to save time/memory for fitting threshold\n",
    "X_train_sub = create_windows(df_train.iloc[-50000:], config['window_size'], stride=5, feature_cols=config['features'])\n",
    "\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"X_train (Subset): {X_train_sub.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline & Threshold Calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "# 1. Reconstruction Threshold (Percentile 99.9)\n",
    "train_recon_errors = get_reconstruction_errors(model, X_train_sub)\n",
    "THRESHOLD_RECON = np.percentile(train_recon_errors, 99.9)\n",
    "print(f\"Reconstruction Threshold (99.9%): {THRESHOLD_RECON:.6f}\")\n",
    "\n",
    "# 2. Mahalanobis Threshold (Latent Space)\n",
    "# Get latent vectors for Train\n",
    "tensor_train = torch.from_numpy(X_train_sub.astype(np.float32))\n",
    "loader_train = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(tensor_train), batch_size=256, shuffle=False)\n",
    "train_latents = get_latent_vectors(model, loader_train)\n",
    "\n",
    "MU, INV_COV = fit_mahalanobis(train_latents)\n",
    "# Chi-Square Threshold (p=0.001, df=3)\n",
    "THRESHOLD_MAHA = chi2.ppf(0.999, df=config['latent_dim'])\n",
    "print(f\"Mahalanobis Threshold (Chi2 99.9%): {THRESHOLD_MAHA:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Stress Testing (Monte Carlo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "def run_stress_test(X_test, model, anomaly_type, feature_name, magnitude=0.0, num_samples=100):\n",
    "    feature_idx = config['features'].index(feature_name)\n",
    "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "    results = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        # 1. Prepare Data\n",
    "        original = X_test[idx].copy()\n",
    "        anomalous = X_test[idx].copy()\n",
    "        \n",
    "        # 2. Inject Anomaly\n",
    "        if anomaly_type == 'spike':\n",
    "            mid = config['window_size'] // 2\n",
    "            anomalous[mid:mid+5, feature_idx] += magnitude\n",
    "        elif anomaly_type == 'drift':\n",
    "            drift = np.linspace(0, magnitude, config['window_size'])\n",
    "            anomalous[:, feature_idx] += drift\n",
    "        elif anomaly_type == 'freeze':\n",
    "            # Force a perfect freeze (std=0)\n",
    "            freeze_val = anomalous[0, feature_idx]\n",
    "            anomalous[:, feature_idx] = freeze_val\n",
    "            \n",
    "        # 3. Detection A: Freeze Rule (PLC Logic)\n",
    "        is_freeze = check_freeze(anomalous)\n",
    "        \n",
    "        # 4. Neural Checks (Only if not freeze, or parallel)\n",
    "        # Get Reconstruction & Latent for BOTH Original and Anomalous\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Convert to tensors\n",
    "            batch_orig = torch.from_numpy(original[np.newaxis, ...].astype(np.float32)).to(DEVICE)\n",
    "            batch_anom = torch.from_numpy(anomalous[np.newaxis, ...].astype(np.float32)).to(DEVICE)\n",
    "            \n",
    "            # Forward Pass\n",
    "            recon_orig, _ = model(batch_orig)\n",
    "            recon_anom, latent_anom = model(batch_anom)\n",
    "            \n",
    "            # Errors\n",
    "            mae_normal = torch.mean(torch.abs(batch_orig - recon_orig)).item()\n",
    "            mae_anomaly = torch.mean(torch.abs(batch_anom - recon_anom)).item()\n",
    "            \n",
    "            # SNR Calculation\n",
    "            snr = mae_anomaly / (mae_normal + 1e-9)\n",
    "            \n",
    "            # Mahalanobis\n",
    "            latent_np = latent_anom.cpu().numpy().squeeze()\n",
    "            maha_dist = calculate_mahalanobis(latent_np, MU, INV_COV)\n",
    "            \n",
    "        detected_recon = mae_anomaly > THRESHOLD_RECON\n",
    "        detected_maha = maha_dist > THRESHOLD_MAHA\n",
    "        \n",
    "        # Final Decision: OR Gate\n",
    "        detected_final = is_freeze or detected_recon or detected_maha\n",
    "        \n",
    "        results.append({\n",
    "            'type': anomaly_type,\n",
    "            'feature': feature_name,\n",
    "            'magnitude': magnitude,\n",
    "            'mae_normal': mae_normal,\n",
    "            'mae_anomaly': mae_anomaly,\n",
    "            'snr': snr,\n",
    "            'maha_dist': maha_dist,\n",
    "            'is_freeze': is_freeze,\n",
    "            'detected_recon': detected_recon,\n",
    "            'detected_maha': detected_maha,\n",
    "            'detected': detected_final\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "print(\"Running Sensitivity Analysis (Matching Baseline Benchmark)...\")\n",
    "\n",
    "MAGNITUDES = [0.1, 0.25, 0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "results_list = []\n",
    "\n",
    "for mag in MAGNITUDES:\n",
    "    print(f\"Testing Magnitude: {mag}\")\n",
    "    \n",
    "    # Spike (Tensile Strength)\n",
    "    res_spike = run_stress_test(X_test, model, 'spike', 'dynamic_tensile_strength', magnitude=mag)\n",
    "    results_list.append(res_spike)\n",
    "    \n",
    "    # Drift (Ejection)\n",
    "    res_drift = run_stress_test(X_test, model, 'drift_end', 'ejection', magnitude=mag)\n",
    "    results_list.append(res_drift)\n",
    "\n",
    "# Freeze (Single Pass, Magnitude Irrelevant)\n",
    "print(\"Testing Freeze...\")\n",
    "res_freeze = run_stress_test(X_test, model, 'freeze', 'cyl_main', magnitude=0.0)\n",
    "results_list.append(res_freeze)\n",
    "\n",
    "# Combine\n",
    "all_results = pd.concat(results_list)\n",
    "all_results.to_csv(os.path.join(RESULTS_DIR, \"lstm_sensitivity.csv\"), index=False)\n",
    "print(f\"\\u2705 Saved detailed results to {RESULTS_DIR}/lstm_sensitivity.csv\")\n",
    "\n",
    "# Summary\n",
    "summary = all_results.groupby(['type', 'magnitude']).agg(\n",
    "    Detection_Rate=('detected', lambda x: np.mean(x) * 100),\n",
    "    Avg_SNR=('snr', 'mean'),\n",
    "    Avg_Anomaly_MAE=('mae_anomaly', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(\"=== SENSITIVITY REPORT ===\")\n",
    "print(summary)\n",
    "summary.to_csv(os.path.join(RESULTS_DIR, \"lstm_sensitivity_summary.csv\"), index=False)\n",
    "\n",
    "# Visualize SNR Boxplot (for all magnitudes)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=all_results, x='type', y='snr', hue='magnitude', palette='viridis')\n",
    "plt.axhline(1.0, color='r', linestyle='--', label='No Signal (1.0)')\n",
    "plt.title(\"LSTM Signal-to-Noise Ratio by Anomaly & Magnitude\")\n",
    "plt.yscale('log')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"lstm_snr_sensitivity.png\"))\n",
    "print(\"\\u2705 Saved SNR Plot\")\n",
    "\n",
    "# Visualize Sensitivity Curve (For comparison with Baseline)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=summary[summary['type'] != 'freeze'], x='magnitude', y='Detection_Rate', hue='type', markers=True)\n",
    "plt.axhline(100, color='green', linestyle='--', alpha=0.3)\n",
    "plt.title(\"LSTM Sensitivity Curve\")\n",
    "plt.ylabel(\"Detection Rate (%)\")\n",
    "plt.xlabel(\"Magnitude\")\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(RESULTS_DIR, \"lstm_sensitivity_curve.png\"))\n",
    "print(\"\\u2705 Saved Sensitivity Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary & Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": [
    "summary = all_results.groupby(\"type\").agg(\n",
    "    Detection_Rate=('detected', lambda x: np.mean(x) * 100),\n",
    "    Avg_SNR=('snr', 'mean'),\n",
    "    Avg_Anomaly_MAE=('mae_anomaly', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "print(\"=== STRESS TEST REPORT ===\")\n",
    "print(f\"Reconstruction Threshold (99.9%): {THRESHOLD_RECON:.4f}\")\n",
    "print(f\"Mahalanobis Threshold (Chi2): {THRESHOLD_MAHA:.4f}\")\n",
    "print(summary)\n",
    "\n",
    "# Save Summary JSON\n",
    "summary_dict = {\n",
    "    \"threshold_recon\": float(THRESHOLD_RECON),\n",
    "    \"threshold_maha\": float(THRESHOLD_MAHA),\n",
    "    \"tests\": summary.to_dict(orient=\"records\")\n",
    "}\n",
    "with open(\"stress_test_summary.json\", \"w\") as f:\n",
    "    json.dump(summary_dict, f, indent=4)\n",
    "print(\"\u2705 Saved summary to stress_test_summary.json\")\n",
    "\n",
    "# Visualize SNR\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(data=all_results, x='type', y='snr', palette='Set2')\n",
    "plt.axhline(1.0, color='r', linestyle='--', label='No Signal (1.0)')\n",
    "plt.title(\"Signal-to-Noise Ratio by Anomaly Type\")\n",
    "plt.ylabel(\"SNR (Anomaly Error / Normal Error)\")\n",
    "plt.savefig(\"snr_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# --- 3D LATENT SPACE VISUALIZATION (Ensure it runs) ---\n",
    "print(\"Generating 3D Latent Space Plot...\")\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Get Latents for a subset of Normal Data\n",
    "indices_normal = np.random.choice(len(X_test), 500, replace=False)\n",
    "batch_normal = torch.from_numpy(X_test[indices_normal].astype(np.float32)).to(DEVICE)\n",
    "_, latents_normal = model(batch_normal)\n",
    "latents_normal = latents_normal.cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(latents_normal[:,0], latents_normal[:,1], latents_normal[:,2], c='blue', alpha=0.5, label='Normal')\n",
    "# We don't have anomaly latents here unless we extracted them in loop, but plotting Normal vs Center is good enough for now\n",
    "ax.scatter(MU[0], MU[1], MU[2], c='red', s=200, marker='X', label='Center of Mass')\n",
    "ax.set_title(f\"Latent Space (BottleNeck={config['latent_dim']}D)\")\n",
    "ax.legend()\n",
    "plt.savefig(\"latent_space_3d.png\")\n",
    "print(\"\u2705 Saved latent_space_3d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}