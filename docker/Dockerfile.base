# Etapa 1: Base con Python 3.11 + Java
FROM python:3.11.9-slim as python-base

# Solo lo absolutamente esencial: Java para Spark
RUN apt-get update && \
    apt-get install -y --no-install-recommends\
        openjdk-17-jdk \
        wget \
        && apt-get clean \
        && rm -rf /var/lib/apt/lists/*

# Configura variables de entorno para Java
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
ENV PATH=$PATH:$JAVA_HOME/bin

# Etapa 3: Instalar Jupyter y PySpark
FROM python-base as install-requirements

COPY requirements.txt /tmp/requirements.txt

# Instala solo lo necesario para Jupyter + PySpark
RUN python -m pip install --upgrade pip && \
    python -m pip install -r /tmp/requirements.txt

# Añadir soporte S3A: hadoop-aws y aws-java-sdk-bundle en el paquete pyspark de pip
RUN wget -q -P /usr/local/lib/python3.11/site-packages/pyspark/jars/ \
        https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar

RUN wget -q -P /usr/local/lib/python3.11/site-packages/pyspark/jars/ \
    https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.24.6/bundle-2.24.6.jar


# Etapa 4: Imagen final
FROM install-requirements as final

WORKDIR /app

# Expone el puerto de Jupyter
EXPOSE 8888

# Copia el código de la aplicación
COPY . .

# Comando por defecto para iniciar Jupyter Notebook
CMD ["jupyter", "notebook", "--allow-root", "--ip=0.0.0.0", "--port=8888", "--no-browser"]